services:
  llm-api-for-next-chat:
    build: .
    image: llm-api-for-next-chat/llm-api-for-next-chat:latest
    container_name: llm-api-for-next-chat
    ports:
      - 5000:5000
    restart: unless-stopped
    volumes:
      - ./chatgpt_web/file_cache.json:/app/chatgpt_web/file_cache.json
      - ./theb_ai/Theb_API.json:/app/theb_ai/Theb_API.json
      - ./hugging_chat/config.json:/app/hugging_chat/config.json
      - ./generated_images:/app/generated_images
      - .env:/app/.env
      - ./docker-compose.yml:/app/docker-compose.yml
      - ./scripts:/app/scripts
    networks:
      - llms_api

  chatgpt-next-web:
    image: yidadaa/chatgpt-next-web:v2.15.7
    container_name: chatgpt-next-web
    restart: unless-stopped
    ports:
      - 3030:3000
    environment:
      BASE_URL: http://llm-api-for-next-chat:5000/api/openai
      ANTHROPIC_URL: http://llm-api-for-next-chat:5000/api/anthropic
      DEEPSEEK_URL: http://llm-api-for-next-chat:5000/api/openai/v1
      CUSTOM_MODELS: "-all,\
        o4-mini@OpenAI,\
        o3-mini@OpenAI,\
        o1-preview@OpenAI,\
        o1@OpenAI,\
        o1-mini@OpenAI,\
        gpt-4.1@OpenAI,\
        gpt-4.1-mini@OpenAI,\
        gpt-4o@OpenAI,\
        gpt-4o-mini@OpenAI,\
        gpt-3.5@OpenAI,\
        gemma-2-27b@Gemma,\
        gemma-2-9b@Gemma,\
        learnlm-1.5-pro-experimental@Google,\
        learnlm-2.0-flash-experimental@Google,\
        gemini-2.5-pro@Google,\
        gemini-2.5-flash-preview-04-17@Google,\
        gemini-2.0-flash-exp@Google,\
        gemini-2.0-flash@Google,\
        gemini-2.0-flash-lite@Google,\
        gemini-1.5-pro-latest@Google,\
        gemini-1.5-flash-latest@Google,\
        gemini-1.5-flash-8b-latest@Google,\
        claude-3-7-sonnet@Anthropic,\
        claude-3-7-sonnet-thinking@Anthropic,\
        claude-3-5-sonnet@Anthropic,\
        claude-3-opus@Anthropic,\
        claude-3-sonnet@Anthropic,\
        llama-3.1-405b@Meta,\
        llama-3.1-70b@Meta,\
        llama-3.1-8b@Meta,\
        llama-3-70b@Meta,\
        llama-3-8b@Meta,\
        mixtral-8x22b@MistralAI,\
        mixtral-8x7b@MistralAI,\
        mistral-7b@MistralAI,\
        wizardlm-2-8x22b@WizardLM,\
        dbrx-instruct@Databricks,\
        qwen-2-72b@Qwen,\
        qwen-1.5-110b@Qwen,\
        qwen-1.5-72b@Qwen,\
        qwen-1.5-32b@Qwen,\
        qwen-1.5-14b@Qwen,\
        qwen-1.5-7b@Qwen,\
        deepseek-chat@DeepSeek,\
        yi-34b@01.AI,\
        theb-ai-4.0@TheB.AI,\
        theb-ai@TheB.AI,\
        qwen2.5-72b-instruct@Qwen,\
        llama-3.3-70b-instruct@Meta,\
        c4ai-command-r-plus-08-2024@CohereForAI,\
        llama-3.1-nemotron-70b-instruct-hf@Meta,\
        hermes-3-llama-3.1-8b@Meta,\
        deepseek-r1-distill-qwen-32b@Qwen,\
        qwq-32b@Qwen,\
        gemma-3-27b-it@Gemma,\
        qwen3-235b-a22b@Qwen,\
        mistral-small-3.1-24b-instruct-2503@MistralAI,\
        qwen2.5-vl-32b-instruct@Qwen,\
        phi-4@Microsoft"
    env_file:
      - .env
    networks:
      - llms_api

  monitor:
    image: docker:latest
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - ./docker-compose.yml:/app/docker-compose.yml
      - .env:/app/.env
      - ./scripts:/app/scripts
    command: sh -c "chmod +x /app/scripts/monitor.sh && /app/scripts/monitor.sh"
    depends_on:
      - llm-api-for-next-chat
    networks:
      - llms_api

networks:
  llms_api:
    name: llms_api
